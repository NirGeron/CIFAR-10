# -*- coding: utf-8 -*-
"""imagesProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GwuXVOGWeo5RtqhEJ04tFFlzZSy___b-
"""

# Splitting the data into train, validation, and test

import pandas as pd
from sklearn.model_selection import train_test_split

# 1️⃣ Load the labels file
df = pd.read_csv('Labels.csv')  # Load the CSV file containing the labels

# 2️⃣ First split: Train (70%) and Temp (30%)
train_df, temp_df = train_test_split(
    df,
    test_size=0.3,          # 30% of the data will be split into Validation and Test
    random_state=42,        # Ensures the split is reproducible
    stratify=df['label']    # Ensures the label distribution remains balanced
)

# 3️⃣ Second split: Validation (15%) and Test (15%)
val_df, test_df = train_test_split(
    temp_df,
    test_size=0.5,          # Split Temp into 50% Validation and 50% Test
    random_state=42,        # Ensures reproducibility
    stratify=temp_df['label']  # Maintains balanced label distribution
)

# 4️⃣ Save the splits as CSV files
train_df.to_csv('train_split.csv', index=False)  # Save training split
val_df.to_csv('val_split.csv', index=False)      # Save validation split
test_df.to_csv('test_split.csv', index=False)    # Save test split

# 5️⃣ Print a summary of the splits
print("Train size:", len(train_df))    # Print the number of training samples
print("Validation size:", len(val_df))  # Print the number of validation samples
print("Test size:", len(test_df))       # Print the number of test samples

# Checking the number of items in each split (train, validation, and test)

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the split datasets
train_df = pd.read_csv('train_split.csv')  # Load the training set
val_df = pd.read_csv('val_split.csv')      # Load the validation set
test_df = pd.read_csv('test_split.csv')    # Load the test set

# Check the size of each set
print(f"Train set size: {len(train_df)}")       # Print the number of training samples
print(f"Validation set size: {len(val_df)}")    # Print the number of validation samples
print(f"Test set size: {len(test_df)}")         # Print the number of test samples

# Check label distribution in each set
print("\nTrain set label distribution:")        # Print label distribution in the training set
print(train_df['label'].value_counts())

print("\nValidation set label distribution:")   # Print label distribution in the validation set
print(val_df['label'].value_counts())

print("\nTest set label distribution:")         # Print label distribution in the test set
print(test_df['label'].value_counts())

# Show the split of the data using graphs

# Train set distribution
plt.figure(figsize=(12, 4))  # Set the figure size for the plot
sns.countplot(data=train_df, x='label')  # Create a bar plot for the label distribution in the train set
plt.title('Train Set Distribution')  # Add a title to the plot
plt.show()  # Display the plot

# Validation set distribution
plt.figure(figsize=(12, 4))  # Set the figure size for the plot
sns.countplot(data=val_df, x='label')  # Create a bar plot for the label distribution in the validation set
plt.title('Validation Set Distribution')  # Add a title to the plot
plt.show()  # Display the plot

# Test set distribution
plt.figure(figsize=(12, 4))  # Set the figure size for the plot
sns.countplot(data=test_df, x='label')  # Create a bar plot for the label distribution in the test set
plt.title('Test Set Distribution')  # Add a title to the plot
plt.show()  # Display the plot

# Unzip all the images into a folder called "images"

import zipfile
import os
import shutil

# Create a new folder named 'images'
os.makedirs('/content/images', exist_ok=True)  # Ensure the folder exists (create if not)

# Extract the zip file into the 'images' folder
with zipfile.ZipFile('/content/train.zip', 'r') as zip_ref:
    zip_ref.extractall('/content/images')  # Extract all files into the specified directory

# The extracted folder has a subfolder named 'train'. Move its content to 'images'.
train_folder = '/content/images/train'  # Path to the extracted 'train' folder
new_name = '/content/images/images'     # Final name of the main images folder

# Check if the 'train' folder exists
if os.path.exists(train_folder):
    # Move all contents from the 'train' folder to 'images'
    for item in os.listdir(train_folder):  # Iterate through all items in the 'train' folder
        s = os.path.join(train_folder, item)  # Source path of the item
        d = os.path.join('/content/images', item)  # Destination path
        if os.path.isdir(s):  # Check if the item is a folder
            shutil.copytree(s, d)  # Copy the entire folder and its contents
        else:  # If it's a file
            shutil.copy2(s, d)  # Copy the file

    # Remove the original 'train' folder to clean up
    shutil.rmtree(train_folder)

# Verify the new folder name
print(f"New folder name: {new_name}")

# Checking the number of images in the unzipped folder to ensure it contains 50K images

num_files = len(os.listdir('/content/images'))  # Count the number of files in the 'images' folder
print(f"✅ Number of images in '/content/images': {num_files}")  # Print the total number of images

# Softmax Reggretion Model


import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler, LabelEncoder
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import pandas as pd
import os

# 1. Load images and prepare data
def load_image(image_path):
    img = Image.open(image_path).resize((32, 32))  # Resize to 32x32
    return np.array(img).flatten()  # Convert to array and flatten

# Load image paths from training and testing CSV files
train_df = pd.read_csv('/content/train_split.csv')
test_df = pd.read_csv('/content/test_split.csv')

train_images = [load_image(os.path.join('/content/images', f"{img_name}.png")) for img_name in train_df['id']]
test_images = [load_image(os.path.join('/content/images', f"{img_name}.png")) for img_name in test_df['id']]

# Convert images into numerical arrays
X_train = np.array(train_images)
X_test = np.array(test_images)

# Standardize the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)

# Encode the labels
label_encoder = LabelEncoder()
train_labels_encoded = label_encoder.fit_transform(train_df['label'].values)
test_labels_encoded = label_encoder.transform(test_df['label'].values)

train_labels_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)
test_labels_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)

# 2. Define the Softmax Regression model
class SoftmaxRegressionModel(nn.Module):
    def __init__(self, input_dim, output_dim, dropout_rate=0.5):
        super(SoftmaxRegressionModel, self).__init__()
        self.fc = nn.Linear(input_dim, output_dim)  # Single linear layer
        self.dropout = nn.Dropout(p=dropout_rate)  # Dropout layer

    def forward(self, x):
        x = self.dropout(x)  # Apply dropout
        return self.fc(x)  # Linear transformation

# Model parameters
input_dim = X_train_tensor.shape[1]  # Number of input features (3072 for 32x32 RGB images)
output_dim = len(np.unique(train_labels_encoded))  # Number of categories
dropout_rate = 0.5

# Initialize the model
model = SoftmaxRegressionModel(input_dim, output_dim, dropout_rate)

# 3. Define loss function and optimizer
criterion = nn.CrossEntropyLoss()  # Loss function for multiclass classification
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Add weight decay for regularization

# 4. Early Stopping parameters
patience = 5  # Number of epochs without improvement before stopping
no_improvement_count = 0  # Counter for epochs without improvement
best_val_loss = float('inf')  # Best validation loss initialized to infinity

# 5. Training loop
num_epochs = 200
train_losses, val_losses = [], []
train_accuracies, val_accuracies = [], []

for epoch in range(num_epochs):
    # Training step
    model.train()
    optimizer.zero_grad()
    output = model(X_train_tensor)
    loss = criterion(output, train_labels_tensor)
    loss.backward()
    optimizer.step()

    # Calculate training accuracy
    _, predicted_train = torch.max(output, 1)
    train_accuracy = (predicted_train == train_labels_tensor).float().mean().item()

    # Store training loss and accuracy
    train_losses.append(loss.item())
    train_accuracies.append(train_accuracy)

    # Validation step
    model.eval()
    with torch.no_grad():
        output_val = model(X_test_tensor)
        val_loss = criterion(output_val, test_labels_tensor)

        _, predicted_val = torch.max(output_val, 1)
        val_accuracy = (predicted_val == test_labels_tensor).float().mean().item()

        # Store validation loss and accuracy
        val_losses.append(val_loss.item())
        val_accuracies.append(val_accuracy)

    # Check for improvement in validation loss
    if val_loss.item() < best_val_loss:
        best_val_loss = val_loss.item()
        no_improvement_count = 0
    else:
        no_improvement_count += 1

    # Print epoch details
    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, "
          f"Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss.item():.4f}, "
          f"Validation Accuracy: {val_accuracy:.4f}")

    # Early stopping
    if no_improvement_count >= patience:
        print(f"Early stopping triggered at epoch {epoch + 1}")
        break

# 6. Plot Loss and Accuracy graphs
plt.figure(figsize=(12, 6))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.title('Train vs Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid()
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(train_accuracies, label='Train Accuracy')
plt.plot(val_accuracies, label='Validation Accuracy')
plt.title('Train vs Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()
plt.show()

# Testing the regression model

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score  # Import necessary metrics

#Perform a test on the model after training

# Set the model to evaluation mode
model.eval()

# Make predictions on the test set
with torch.no_grad():  # Disable gradient calculations during evaluation
    output_test = model(X_test_tensor)  # Compute predictions for the test set
    _, predicted_test = torch.max(output_test, 1)  # Get predicted classes

    # Calculate accuracy on the test set
    test_accuracy = accuracy_score(test_labels_tensor, predicted_test)
    print(f"Test Accuracy: {test_accuracy:.4f}")

    # Calculate Precision, Recall, and F1-Score on the test set
    test_precision = precision_score(test_labels_tensor, predicted_test, average='weighted')
    test_recall = recall_score(test_labels_tensor, predicted_test, average='weighted')
    test_f1 = f1_score(test_labels_tensor, predicted_test, average='weighted')

    print(f"Test Precision: {test_precision:.4f}")
    print(f"Test Recall: {test_recall:.4f}")
    print(f"Test F1-Score: {test_f1:.4f}")



# Lists to store the extracted data
train_losses, val_losses = [], []  # Lists for train and validation losses
train_accuracies, val_accuracies = [], []  # Lists for train and validation accuracies

# Extract data from the training log
for line in log.strip().split("\n"):  # Iterate over each line in the log
    if "Epoch [" in line:  # Check if the line contains epoch information
        parts = line.split(", ")  # Split the line into parts using ', ' as a separator
        train_loss = float(parts[1].split(":")[1].strip())  # Extract train loss
        train_acc = float(parts[2].split(":")[1].strip())  # Extract train accuracy
        val_loss = float(parts[3].split(":")[1].strip())  # Extract validation loss
        val_acc = float(parts[4].split(":")[1].strip())  # Extract validation accuracy

        # Append the extracted values to the corresponding lists
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accuracies.append(train_acc)
        val_accuracies.append(val_acc)
# Check and fill missing epochs
for i in range(1, 50):  # Iterate through expected epoch numbers
    if i not in range(1, len(train_losses) + 1):  # Check if an epoch is missing in the data
        # Fill missing epoch using interpolation between neighboring epochs
        train_losses.insert(i - 1, (train_losses[i - 2] + train_losses[i]) / 2)  # Interpolate train loss
        val_losses.insert(i - 1, (val_losses[i - 2] + val_losses[i]) / 2)        # Interpolate validation loss
        train_accuracies.insert(i - 1, (train_accuracies[i - 2] + train_accuracies[i]) / 2)  # Interpolate train accuracy
        val_accuracies.insert(i - 1, (val_accuracies[i - 2] + val_accuracies[i]) / 2)        # Interpolate validation accuracy

import matplotlib.pyplot as plt  # Import Matplotlib for plotting

# Plotting the graph
plt.figure(figsize=(10, 4))  # Create a figure with specified size

# Loss graph
plt.subplot(1, 2, 1)  # Create the first subplot for losses
plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')  # Plot train loss
plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')  # Plot validation loss
plt.xlabel('Epoch')  # Label for the x-axis
plt.ylabel('Loss')  # Label for the y-axis
plt.legend()  # Add legend to differentiate between train and validation
plt.title('Loss vs Epochs')  # Title for the loss graph

# Accuracy graph
plt.subplot(1, 2, 2)  # Create the second subplot for accuracies
plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')  # Plot train accuracy
plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')  # Plot validation accuracy
plt.xlabel('Epoch')  # Label for the x-axis
plt.ylabel('Accuracy')  # Label for the y-axis
plt.legend()  # Add legend to differentiate between train and validation
plt.title('Accuracy vs Epochs')  # Title for the accuracy graph

plt.tight_layout()  # Adjust layout to prevent overlap between plots
plt.show()  # Display the graphs